_target_: lightning.pytorch.Trainer
precision: "32-true"                # other options are 32, 64, 'bf16-mixed', '16-mixed', currently only support '32float'
# the grad is accumulated through batches to simulate larger batch size, 
# for instance if it is set to 4, and the batch size is 4, the equivalent 
# batch size will be 4*4=16, so now 1 means that nothing changed here
accumulate_grad_batches: 1
max_epochs: ${general.max_epochs}    
log_every_n_steps: 5